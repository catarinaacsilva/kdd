{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Image Processing\n",
    "\n",
    "## Classifying acanthocytes using image processing and ML techniques: A comparative study\n",
    "\n",
    "\n",
    "The diagnosis of several diseases can be improved with the identification of acanthocytes, i.e., red blood cells with abnormal form. The following paper describes an approach to autonomously identify such cells in blood sample images: [Classifying acanthocytes using image processing and ML techniques: A comparative study](https://www.researchgate.net/publication/345003926_Classifying_acanthocytes_using_image_processing_and_ML_techniques_A_comparative_study).\n",
    "\n",
    "The method relies on image processing operations and conventional machine learning methods. The principal motivation is the fact that this identification is usually performed by specialized devices or done manually by humans. Specialized devices are rare and costly, while manual identification is prune to error. Our approach reaches a precision of 91%, showing the potential of the solution.\n",
    "\n",
    "The main goal is to develop a reliable detection and classification procedure for acanthocytes, using a reduced set of features. The first step is to apply image procesing techniques and after this, the step is to apply ML algorithms.\n",
    "\n",
    "Image processing techniques are used to segment blood cells and conventional ML models to classify them. The output is the classification of each blood cell into one of two classes: normal cells or acanthocytes. Additionally, the number of acanthocytes in the blood sample is computed.\n",
    "\n",
    "## Describing dataset\n",
    "\n",
    "The code that generated dataset is publicly available [here](https://github.com/catarinaacsilva/medical-image-processing).\n",
    "\n",
    "The first step is to normalize an input image by converting it to gray scale and applying a 9x9 median filter to smooth noise. \n",
    "The gray image is then converted to binary using the Otsu thresholding method. \n",
    "Those operations may originate some holes in the middle of the cells and medium-sized noise (by-product of the binarization).\n",
    "\n",
    "The next steps fix that by executing a filling operation (imfill) that applies a guided flooding operation to close holes inside blobs.\n",
    "Morphological reconstruction (elliptic shaped 9x9 kernel) is applied to remove the medium-sized noise produced during the binarization. Finally, the Canny edge detector is applied to extract region contours.\n",
    "\n",
    "![](images/img00.png)\n",
    "\n",
    "Based on the extracted region contours, we compute several features that describe them.\n",
    "The first feature is the histogram from the chain code. A chain code characterizes the shape of a contour but is not rotation and scale invariant. To achieve that we compute an histogram with the relative weight for each direction of the chain code.\n",
    "The remaining features are circularity, roundness, aspect-ratio and solidity.\n",
    "The previously mentioned features are shape descriptors commonly used by image processing toolboxes to classify blobs.\n",
    "These features are meant to enhance the classification process, by expanding the expressiveness of the histogram, and capture characteristics that are invariant to scale and rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load arff file\n",
    "\n",
    "import arff, numpy as np\n",
    "dataset = arff.load(open('dataset/medical_image.arff', 'r'))\n",
    "data = np.array(dataset['data'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into input (X) and output (y) variables\n",
    "\n",
    "X = data[:, :-1]\n",
    "X = X.astype(np.float64)\n",
    "y = data[:,-1]\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "\n",
    "\n",
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "  \n",
    "# label_encoder object knows how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "\n",
    "y= label_encoder.fit_transform(y)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# Define the cutoff for best features\n",
    "k=3\n",
    "\n",
    "X = SelectKBest(chi2, k=k).fit_transform(X, y)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "\n",
    "k_fold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "acc = np.array([])\n",
    "precision = np.array([])\n",
    "recall = np.array([])\n",
    "f1 = np.array([])\n",
    "mcc = np.array([])\n",
    "\n",
    "classifiers = [('svm', SGDClassifier(loss=\"log\")), \n",
    "              ('perceptron', Perceptron(penalty=None, alpha=0.0001, fit_intercept=True, tol=None, \n",
    "               eta0=0.1, n_jobs=1, random_state=0, class_weight=None, warm_start=False)),\n",
    "              ('knn1', KNeighborsClassifier(n_neighbors=1)),\n",
    "              ('knn3', KNeighborsClassifier(n_neighbors=3)),\n",
    "              ('knn5', KNeighborsClassifier(n_neighbors=5)),\n",
    "              ('decision tree', DecisionTreeClassifier()),\n",
    "              ('random forest', RandomForestClassifier())]\n",
    "for name, model in classifiers:\n",
    "    for train_indices, test_indices in k_fold.split(X,y):\n",
    "        scaler = StandardScaler() \n",
    "        X_train=X[train_indices]\n",
    "\n",
    "        Xs=scaler.fit_transform(X_train)\n",
    "\n",
    "        Xtest=scaler.transform(X[test_indices])\n",
    "\n",
    "        model.fit(Xs,y[train_indices])\n",
    "\n",
    "        y_pred=model.predict(Xtest)\n",
    "        cm = confusion_matrix(y[test_indices],y_pred)\n",
    "        #print(cm)\n",
    "\n",
    "        tp = cm[1][1]\n",
    "        tn = cm[0][0]\n",
    "        fp = cm[0][1]\n",
    "        fn = cm[1][0]\n",
    "\n",
    "        #Accuracy\n",
    "        acc = np.append(acc, (tp+tn)/(tp+tn+fp+fn))\n",
    "\n",
    "        # Precision\n",
    "        precision = np.append(precision, tp/(tp+fp))\n",
    "\n",
    "\n",
    "        #Recall\n",
    "        recall = np.append(recall, tp/(tp+fn))\n",
    "\n",
    "\n",
    "        #F1 Score\n",
    "        f1 = np.append(f1, (2*tp)/(2*tp + fp + fn))\n",
    "\n",
    "\n",
    "        #MCC\n",
    "        mcc =np.append(mcc, (tp*tn-fp*fn)/math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    print('Model: ', name) \n",
    "    print('Accuracy = ', np.average(acc))\n",
    "    print('Precision = ', np.average(precision))\n",
    "    print('Recall = ', np.average(recall))\n",
    "    print('F1 = ', np.average(f1))\n",
    "    print('Matthews correlation coefficient = ', np.average(mcc))\n",
    "    print('********************************************************************************')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

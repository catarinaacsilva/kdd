{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Dataset - Homework exercice 2 (Programming task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this work we will analyse the dataset named [MAGIC Gamma Telescope](https://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope). \n",
    "The data are MC generated to simulate registration of high energy gamma particles in a ground-based atmospheric Cherenkov gamma telescope using the imaging technique. Cherenkov gamma telescope observes high energy gamma rays, taking advantage of the radiation emitted by charged particles produced inside the electromagnetic showers initiated by the gammas, and developing in the atmosphere. \n",
    "\n",
    "The main goal is to apply features selection methods to properly prepare the dataset to train a model.\n",
    "\n",
    "One of the greater challenge in machine learning is selecting the best features to train the model. We need only the features which are highly dependent on the output variable.\n",
    "\n",
    "\n",
    "\n",
    "### Authors:\n",
    "\n",
    "- Catarina Silva\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "#Import Numpy library\n",
    "import numpy as np\n",
    "\n",
    "#Import Plotly library\n",
    "import plotly.express as px\n",
    "\n",
    "#Import Matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import Seaborn library\n",
    "from pylab import rcParams\n",
    "import seaborn as sb\n",
    "\n",
    "#Import Scipy's Pearson Correllation function\n",
    "import scipy\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "#Import SKlearn Scaler, PCA & KPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define plot libraries settings\n",
    "#Python magic function that allows plot inside Jupyter Notebook\n",
    "%matplotlib inline \n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "sb.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Center notebook images\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './dataset/magic04.data'\n",
    "\n",
    "dataset = pd.read_csv(file)\n",
    "\n",
    "dataset.columns = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'f3MTrans', 'fAlpha', 'fDist', 'class']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(dataset, hue='class', diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sb.pairplot(dataset, hue='class', diag_kind=\"kde\", kind = 'reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Pearson’s correlation to detect linear correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = dataset.corr(method='pearson').abs()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sb.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot=True, cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Threshold for cutoff\n",
    "th = 0.75\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > th)]\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = dataset.drop(dataset[to_drop], axis=1).corr(method='pearson').abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot=True, cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson Correlation is a statistic that measures linear correlation between two features. It has a value between +1 and −1. A value of +1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation.\n",
    "\n",
    "This correlation can be used to identify pairs of linear correlated features.\n",
    "Since the features are linear correlated (increase/decrease at similar rates) having both in the dataset does not offer additional information.\n",
    "\n",
    "This method can be used to removed higly correlated features, however it does not provide any information regarding the relation between the the input feature and the target output.\n",
    "As such, it is not possible to identify the most 2 discriminative features with this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA f-test feature ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) helps us to select the best features.\n",
    "ANOVA is a parametric statistical hypothesis test for determining whether the means from two or more samples of data (often three or more) come from the same distribution or not.\n",
    "\n",
    "An F-statistic, or F-test, is a class of statistical tests that calculate the ratio between variances values, such as the variance from two different samples or the explained and unexplained variance by a statistical test, like ANOVA. The ANOVA method is a type of F-statistic referred to here as an ANOVA f-test.\n",
    "\n",
    "Importantly, it is used when one variable is numeric and one is categorical, such as numerical input variables and a classification target variable in a classification task.\n",
    "\n",
    "The results of this test can be used for feature selection where those features that are independent of the target variable can be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and summarize the dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Funtion that loads the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    data = pd.read_csv(filename, header=None)\n",
    "    # retrieve numpy array\n",
    "    dataset = data.values\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1]\n",
    "    return X, y\n",
    "\n",
    "# Load the dataset\n",
    "X, y = load_dataset('./dataset/magic04.data')\n",
    "\n",
    "# Split into train and test sets (split 70/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Summarize the dataset\n",
    "print('Train', X_train.shape, y_train.shape)\n",
    "print('Test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA f-test feature selection for numerical data\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from matplotlib import pyplot\n",
    "import heapq\n",
    "\n",
    "# Function that applies feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select all features\n",
    "    fs = SelectKBest(score_func=f_classif, k='all')\n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform train input data\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    # transform test input data\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# Feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
    "\n",
    "# The rank of each feature\n",
    "for i in range(len(fs.scores_)):\n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "\n",
    "# plot the ranks\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "pyplot.show()\n",
    "\n",
    "# Define the cutoff for best features\n",
    "k=2\n",
    "\n",
    "# Print K best features\n",
    "idx = heapq.nlargest(k, range(len(fs.scores_)), fs.scores_.__getitem__)\n",
    "print(\"The {} top features are:\".format(k))\n",
    "for i in idx:\n",
    "    print(\"Feature {}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of feature selection is to identify a reduced set of features that are discrimative (are provide enough information to properly separate the target output) but reduce the computational time or even improve accuracy.\n",
    "This method ranks the relation of each feature with the target output (a higher rank implies that the feature is higly related with the target output).\n",
    "From the rank of the ANOVA f-test the two most disrciminative features are 8 and 0.\n",
    "\n",
    "However it is not possible to state that using only these two features are sufficiently to train the model. The third most discriminative feature (number 1) is almost similar (in rank) to feature 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the performance of a model only benefits from more features up until a certain point.\n",
    "The more features are fed into a model, the more the dimensionality of the data increases.\n",
    "As the dimensionality increases, overfitting becomes more likely.\n",
    "\n",
    "Not only can high dimensionality lead to long training times, more features often lead to an algorithm overfitting as it tries to create a model that explains all the features in the data.\n",
    "\n",
    "There are multiple techniques that can be used to fight overfitting, but dimensionality reduction is one of the most effective techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical Features\n",
    "features = dataset.columns\n",
    "features = features.delete(len(features)-1)\n",
    "\n",
    "# Separating out the features\n",
    "x = dataset.loc[:, features].values\n",
    "\n",
    "# Separating out the target\n",
    "y = dataset.loc[:,['class']].values\n",
    "\n",
    "# Color of the output\n",
    "color=[]\n",
    "for out in y:\n",
    "    if out == 0:\n",
    "        color.append('#1f77b4')\n",
    "    else:\n",
    "        color.append('#bcbd22')\n",
    "\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA nº columns\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "#Get Best Features throw PCA\n",
    "principalComponents = pca.fit_transform(x)\n",
    "\n",
    "#Resolve info into DataFrame\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['Best Feature 1', 'Best Feature 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append Categorical Feature\n",
    "finalDf = pd.concat([principalDf, dataset[['class']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame\n",
    "print(finalDf)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(principalComponents[:,0], principalComponents[:,1], c=color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot it is possible to see the dataset reduced to only two features.\n",
    "Each axis represent one of those features and the dots are the values of each feature and the color of the dot represents the class.\n",
    "\n",
    "It is possible to see that there is a large area of overlapping between the blue and green dots, as such the reconstruction using only two feature (with PCA) is not sufficient discriminative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KPCA nº columns\n",
    "kpca = KernelPCA(n_components=2, kernel='linear')\n",
    "\n",
    "#Get Best Features throw KPCA\n",
    "transformer  = kpca.fit_transform(x)\n",
    "\n",
    "#Resolve info into DataFrame\n",
    "kpcaDf = pd.DataFrame(data = transformer\n",
    "             , columns = ['Best Feature 1', 'Best Feature 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append Categorical Feature\n",
    "kpcaDf = pd.concat([kpcaDf, dataset[['class']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show DataFrame\n",
    "print(kpcaDf)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(transformer[:,0], transformer[:,1], c=color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot it is possible to see the dataset reduced to only two features.\n",
    "Each axis represent one of those features and the dots are the values of each feature and the color of the dot represents the class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPCA (RBF kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KPCA nº columns\n",
    "rbf_kpca = KernelPCA(n_components=2, kernel='rbf')\n",
    "\n",
    "#Get Best Features throw KPCA\n",
    "transformer  = rbf_kpca.fit_transform(x)\n",
    "\n",
    "#Resolve info into DataFrame\n",
    "rbf_kpcaDf = pd.DataFrame(data = transformer\n",
    "             , columns = ['Best Feature 1', 'Best Feature 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append Categorical Feature\n",
    "rbf_kpcaDf = pd.concat([rbf_kpcaDf, dataset[['class']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show DataFrame\n",
    "print(rbf_kpcaDf)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(transformer[:,0], transformer[:,1], c=color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot is similar to the ones depicted previously.\n",
    "\n",
    "Contrary to PCA and KPCA(linear) this shows less overlappings area between the blue and green dots.\n",
    "As such, these two features provide more information than the reconstruction of PCA KPCA(linear)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
